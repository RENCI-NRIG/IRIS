We model our problem as a multi-class classification problem where the labels are defined as all the end host nodes and network interfaces that may incur integrity errors. 
The features are flow level characteristics that include source, destination, size, transfer time, throughput, whether a flow is corrupted, missed, or retried, etc. In general, the training process takes 
as input two arrays: an array $X$ of size $[n_{samples}, n_{features}]$ holding the training samples, and an array $y$ of class labels of size $[n_{samples}]$. 
The total number of labels $L$ equals to the number of the link interfaces and the end hosts in the network. 

We say a file transfer succeeds when it incurs no integrity errors. In this study, we focus on RCA analysis with failure data only, \ie, the data sets used in training only contains those corrupted files transfers with fault labels that are extracted from the raw data collected from the experiment that consists of all data transfers. This non-probabilistic approach not only bears its own technical merit but also reflects a realistic situation where only failure events are reported in applications in order to reduce the measurement overhead and storage cost. Due to the low probability nature of the integrity errors, it is not economically viable to save all data transfer monitoring statistics as the majority of them are successful flows. This not only means that a large amount of data transfer flows with good network coverage are indispensable, but also suggests efficient fault injection mechanism is needed, to generate sufficient labeled training data.

There are a large number of different supervised ML models and associated parameter tuning methods. Our data sets introduce mixed numerical and categorical features as well as data imbalance. 
In addition, we want to obtain the class membership probability estimates, not just the most likely single class in the result. We evaluated several model variants using the popular Scikit-learn library~\cite{Scikit:web},  
and found the random forest model results in the best performance in terms of RCA accuracy and training time. 
Decision tree is a natural choice to multi-class classification as the multiple leaves represent the labels. It supports a $predict\_proba$ method that gives the class membership probability estimates. 
Its main advantages include fast prediction time and excellent model explainability. In this study, we tried several ensemble methods based on randomized decision trees or random forests. 
By fitting over multiple randomized decision trees built from randomized samples, the random forest model with the right hyper parameters achieves higher accuracy and controls overfitting better. 

\subsection{Aggregated flows and inference accuracy}
One key observation is that an erratic link may cause integrity errors on all data transfer paths traversing it. While the training data is collected in the form of individually labeled flows, the inference can be done in the unit of all flows that are corrupted at a time since we only consider the single failure scenario. The models are trained with the labeled flow data.  For inference, we consider two different methods: {\it Flow} that just uses individual flow as the input and {\it Aggregated Flow} for which we generated a new data set that all corrupted file transfer flows at a time are aggregated as one input data sample. In the former case, the accuracy is computed on a per flow base. In the latter case, if all the flows in a data set are labeled by $L$ labels, they will be aggregated into $L$ samples to be tested against the trained model. The total number of correct label inference divided by $L$ is defined as the accuracy.

\subsection{Top-$k$ classification accuracy} 
Since we assume training data from data transfer flows only between the end hosts, it doesn't satisfy the necessary condition presented in~\cite{netbouncer:nsdi18}. The conventional classification on a single label inference from the training models performs relatively poorly in terms of accuracy and F-score. In practice, it would be very useful if the model can produce a small set of highly likely causes for the operators to zoom in. Most of the ML models, when used to infer a test sample, actually generate the probability distribution over all the classes. Therefore we can use a Top-$k$ Accuracy metric in evaluation, for which a prediction is defined as correct as long as the set of $k$ labels of highest probability in the classification results contains the correct label of the sample. Both decisions tree and BN models natively support the classification probability output. 

\subsection{Features}
As explained on Equation~\ref{eq:prob}, for a data sample, the feature set could consist of both path features and file transfer features. In our model, we only consider the features that are possible for the application to collect at the end hosts.
So only the end host information is included for the path features because we assume the other network elements on the file transfer paths are unknown. 
The file transfer features include both numerical characteristics like file size and transfer throughput , and categorical features like correctness of integrity checksum and presence of retransmission. 

The impacts of the file transfer features are two-folded. On one hand, the bigger file size may incur a higher probability of file corruption and lower throughput may imply more TCP retransmission caused by corrupted packets, which may help with the RCA performance. On the other hand, different machine learning models perform differently when dealing with a mixture of numerical and categorical features. In reality, there are always engineering and policy limits on obtaining certain 
features for application users in a network. Therefore it is important to study the model performance when only a subset of features are available. Therefore we study two scenarios of different feature sets: {\it No File Features} when the numerical file size and transfer throughput information is not available and {\it All Features} when it is available.

\subsection{Error Asymmetry and Data Imbalance} 
\label{sub:ml:imbalance}
A leading factor that affects the performance of multi-class classification models is the data set imbalance. When data samples from certain classes (called {\it majority classes}) outnumber those from other classes, the trained models will be highly skewed toward the majority classes, which will significantly lower the prediction accuracy. By the nature of integrity error simulation, the file transfer failures caused by faulty network interfaces are more frequent than those caused by the faulty end hosts. And between the two interfaces on a link, the one on the receiving side of a file transfer over TCP has a much higher chance to corrupt the file than the one on the transmitting side. Therefore the raw data we collected is oversampled on a subset of the network interface classes and significantly undersampled on the end host classes. 

\subsection{Classification Granularity} 
\label{sub:ml:granularity}
The wide area network system may cover multiple administrative domains. For example, a subset of core routers or border routers may belong to one service provider and another subset of routers and end hosts may belong to a campus site or a data center. It would be very useful if RCA can localize the failure to a particular domain accurately so the domain administrators can further locate the faulty components with more powerful debugging tools. For the classification model training and inference, this means to aggregate multiple related labels to form {\it super labels} according to certain criteria. This idea could lead an efficient multi-granularity classification framework, which is not difficult to implement on top of the base model, but could be very appreciated in reality. As pointed out by the reference, inefficiency of traditional RCA approaches largely comes from the so called {\it blame game}, \ie, back and forth communication and reasoning between administrators from different domains to identify who is responsible for the tedious debugging in her domain. 





