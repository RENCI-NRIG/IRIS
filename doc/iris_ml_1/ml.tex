A network system can be represented as a simple graph $G(V,E)$ where $V$ is the set of nodes that includes $H$ end hosts and $R$ routers. $E$ is the set of links. 
In general, the fundamental network RCA problem is to use the path-level measurement to deduce the faulty components that can be concisely modeled by the following formula.

\begin{flalign}\label{eq:prob}
\begin{aligned}
&Probability(Packet\ Loss\ in\ Path \ i) =\\
&f(\prod_{j \in P_i}Probability(component\ j\ is\ faulty) )
\end{aligned}
\end{flalign}

The path level statistics can be measured but the component states are unknown that need to be deduced. As shown in previous figures, the result is not deterministic if the path coverage (node pairs) is not sufficient. 
Most of the existing work assume complete information of the network topology and the routing matrix so above path-component relationship can be established and rely on tomography-like continuous 
probing and measurement to generate the path level statistics. Then variants of stochastic learning or statistical significance testing techniques are used to identify the most likely faulty links, where both inputs 
and outputs are continuous variables. One major challenge is the scalability as they need to quickly localize the failure(s) in a network of thousands of nodes or more, that needs efficient regression model optimization 
algorithms (e.g., regularization and gradient methods){~\cite{netbouncer:nsdi18, NetPoirot:Sigcomm2016, DeepView:NSDI18}.

As discussed in previous sections, our problem is different in which the components in the network are known (the possible root causes) but the path routes are unknown and our measurement is per flow (file) rather than per path. 
The file level measurement and characteristics give us extra data features that could enhance the model accuracy, which is the case that we'll show in the evaluation. We therefore use the following modified formula to describe our RCA problem.

\begin{flalign}\label{eq:prob}
\begin{aligned}
&Probability(File\ i\ corrupted) =\\
&f(F_i, \prod_{\tilde{j} \in P_i}Probability(device\ \tilde{j}\ is\ faulty) )
\end{aligned}
\end{flalign}

A path $P_i$ that a file transfer (flow) $i$ traverses consists of origin node $H_i^o$, a set of links where each $e_i^{\tilde{j}}\in E$ has two interfaces $e_i^t$ and $e_i^r$ on the route, and destination node $H_i^d$. 
As our main concern is if a file is corrupted rather than packet losses, the file characteristics, $F_i$, \eg, the source and destination nodes, file size, transfer time, retransmission, etc. all become part of the data features. 
Some of the features are continuous variables and some are categorical variables.
As the components ($\tilde{j}$) of $P\ i$ that file transfer $i$ traverses is unknown except for its two end hosts $H_i^o$ and $H_i^d$, the model actually needs to implicitly learn the file routes in order to infer the root causes.   
As a result, we can not use the traditional stochastic learning approach.

We model our problem as a multi-class classification problem where the labels are defined as all the end host nodes and network interfaces that may incur integrity errors. 
The features are flow level characteristics that include source, destination, size, transfer time, throughput, whether a flow is corrupted, missed, or retried, etc. In general, the training process takes 
as input two arrays: an array $X$ of size $[n_{samples}, n_{features}]$ holding the training samples, and an array $y$ of class labels of size $[n_{samples}]$. 
The total number of labels $L$ equals to the number of the link interfaces and the end hosts in the network. 

We say a file transfer succeeds when it incurs no integrity errors. In this study, we focus on RCA analysis with failure data only, \ie, the data sets used in training only contains those corrupted files transfers with fault labels that are extracted from the raw data collected from the experiment that consists of all data transfers. This non-probabilistic approach not only bears its own technical merit but also reflects a realistic situation where only failure events are reported in applications in order to reduce the measurement overhead and storage cose. Due to the low probability nature of the integrity errors, it is not economically viable to save all data transfer monitoring statistics as the majority of them are normally successful.  A large amount of data transfer flows are needed to generate sufficient training data since our targeted grey failure, the integrity error, has a very low probability (often in the order of $10^{-3}$).

There are a large number of different supervised ML models and associated parameter tuning  methods. Our data sets introduce mixed numerical and categorical features as well as data imbalance. 
In addition, we want to obtain the class membership probability estimates, not just the most likely single class in the result. We evaluated several model variants using the popular Scikit-learn library~\cite{Scikit:web},  
and found the random forest model results in the best performance in terms of RCA accuracy and training time. For the sake of comparison, we also present the results from the popular Bayesian Model.

Decision tree is a natural choice to multi-class classification as the multiple leaves represent the labels. It supports a $predict\_proba$ method that gives the class membership probability estimates. 
Its main advantages include fast prediction time and excellent model explainability. In this study, we tried several ensemble methods based on randomized decision trees or random forests. 
By fitting over multiple randomized decision trees built from randomized samples, the random forest model with the right hyper parameters achieves higher accuracy and controls overfitting better. 

Among a number of models with different kernels, we chose the Multinomial Naive Bayes method, which is suitable for multi-class classification with discrete features. Again its $predict\_proba$ method gives the class membership probability estimates.

\subsection{Aggregated flows and inference accuracy}
One key observation is that an erratic link may cause integrity errors on all data transfer paths traversing it. While the training data is collected in the form of individually labeled flows, the inference can be done in the unit of all flows that are corrupted at a time since we only consider the single failure scenario. The models are trained with the labeled flow data.  For inference, we consider two different methods: {\it Flow} that just uses individual flow as the input and {\it Aggregated Flow} for which we generated a new data set that all corrupted file transfer flows at a time are aggregated as one input data sample. In the former case, the accuracy is computed on a per flow base. In the latter case, if all the flows in a data set are labeled by $L$ labels, they will be aggregated into $L$ samples to be tested against the trained model. The total number of correct label inference divided by $L$ is defined as the accuracy.

\subsection{Top-$k$ classification accuracy} 
Since we assume training data from data transfer flows only between the end hosts, it doesn't satisfy the necessary condition presented in~\cite{netbouncer:nsdi18}. The conventional classification on a single label inference from the training models performs relatively poorly in terms of accuracy and F-score. In practice, it would be very useful if the model can produce a small set of highly likely causes for the operators to zoom in. Most of the ML models, when used to infer a test sample, actually generate the probability distribution over all the classes. Therefore we can use a Top-$k$ Accuracy metric in evaluation, for which a prediction is defined as correct as long as the set of $k$ labels of highest probability in the classification results contains the correct label of the sample. Both decisions tree and BN models natively support the classification probability output. 

\subsection{Features}
As explained on Equation~\ref{eq:prob}, for a data sample, the feature set could consist of both path features and file transfer features. In our model, we only consider the features that are possible for the application to collect at the end hosts.
So only the end host information is included for the path features because we assume the other network elements on the file transfer paths are unknown. 
The file transfer features include both numerical characteristics like file size and transfer throughput , and categorical features like correctness of integrity checksum and presence of retransmission. 

The impacts of the file transfer features are two-folded. On one hand, the bigger file size may incur a higher probability of file corruption and lower throughput may imply more TCP retransmission caused by corrupted packets, which may help with the RCA performance. On the other hand, different machine learning models perform differently when dealing with a mixture of numerical and categorical features. In reality, there are always engineering and policy limits on obtaining certain 
features for application users in a network. Therefore it is important to study the model performance when only a subset of features are available. Therefore we study two scenarios of different feature sets: {\it No File Features} when the numerical file size and transfer throughput information is not available and {\it All Features} when it is available.

\subsection{Error Asymmetry and Data Imbalance} 
\label{sub:ml:imbalance}
A leading factor that affects the performance of multi-class classification models, is the data set imbalance. When data samples from certain classes (called {\it majority classes}) outnumber those from other classes, the trained models will be highly skewed toward the major classes, which will significantly lower the prediction accuracy. By the nature of integrity error simulation, the file transfer failures caused by faulty network interfaces are more frequent than those caused by the faulty end hosts. And between the two interfaces on a link, the one on the receiving side of a file transfer over TCP has a much higher chance to corrupt the file than the one on the transmitting side. Therefore the raw data we collected is oversampled on a subset of the network interface classes and significantly undersampled on the end host classes. 

\subsection{Classification Granularity} 
\label{sub:ml:granularity}
The wide area network system may cover multiple administrative domains. For example, a subset of core routers or border routers may belong to one service provider and another subset of routers and end hosts may belong to a campus site or a data center. It would be very useful if RCA can localize the failure to a particular domain accurately so the domain administrators can further locate the faulty components with more power debugging tools. For the classification model training and inference, this means to aggregate multiple related labels to form {\it super labels} according to certain criteria. This idea could lead an efficient multi-granularity classification framework, which is not difficult to implement on top of the base model, but could be very appreciated in reality. As pointed out by the reference, inefficiency of traditional RCA approaches largely comes from the so called {\it blame game}, \ie, back and forth communication and reasoning between administrators from different domains to identify who is responsible for the tedious debugging in her domain. 





