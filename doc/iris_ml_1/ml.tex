Due to the scalability challenge, some recent work in large scale data center networks (possibly thousands of nodes) adopted stochastic learning approaches in localizing tomography-based performance downgrades or probabilistic grey failure issues. Basically, they attempted to learn an estimate regression model between the output variables in the end-to-end performance measurements and the root causes as the input variables in either performance degradation at the end hosts~\cite{NetPoirot:Sigcomm2016} or gray failure probabilities inside the network nodes or links~\cite{netbouncer:nsdi18,Link-JIoT-2019}. Both inputs and outputs are explicitly defined as continuous variables. Their technical challenges are the regression model optimization algorithms (e.g., regularization and gradient methods) and statistical significance test~\cite{DeepView:NSDI18}. 

Our targeted networks are of multi-domain nature where the RCA granularity can be limited to individual domains instead of individual routers inside a domain, which are totally unknown to the outside. The network can be represented as a simple graph $G(V,E)$ where $V$ is the set of nodes that includes $H$ end hosts and $R$ routers. $E$ is the set of links. We say a file transfer succeeds when it incurs no integrity errors. 

In general, the problem at hand can be concisely represented by the following formula.
\begin{flalign}\label{eq:prob}
\begin{aligned}
&Probability(File\ i\ succeeds) =\\
&f(F_i, \prod_{j \in P_i}Probability(component\ j\ is\ normal) )
\end{aligned}
\end{flalign}

A path $P_i$ that a file transfer (flow) $i$ traverses consists of origin node $H_i^o$, a set of links where each $e_i^j\in E$ has two interfaces $e_i^t$ and $e_i^r$ on the route, and destination node $H_i^d$. As our main concern is if a file is corrupted rather than packet losses, the file characteristics, $F_i$, eg, the file size, transfer time, etc. may play an important role. We emphasize again that the components ($j$) of $P\ i$ that file transfer $i$ traverses is unknown except for its two end hosts $H_i^o$ and $H_i^d$. And a large amount of data transfer flows are needed to generate sufficient training data since our targeted grey failure, the integrity error, has a very low probability (often in the order of $10^{-3}$).

We model our problem as a multi-class classification problem where the labels are defined as all the nodes and links that may incur integrity errors and the features are flow level characteristics that include source, destination, size, transfer time, throughput, whether a flow is corrupted, missed, or retried, etc. In general, the training process takes as input two arrays: an array $X$ of size $[n_{samples}, n_{features}]$ holding the training samples, and an array $y$ of class labels of size $[n_{samples}]$. The total number of labels $L$ equals to the number of the link interfaces and the end hosts in the topology.

In this study, we focus on RCA analysis with failure data only, \ie, the data sets used in training only contains those corrupted files transfers with fault labels that are extracted from the raw data collected from the experiment that consists of both successful data transfers. This non-probabilistic approach not only bears its own technical merit but also reflects the reality that only failure events are going to be reported in applications. Due to the low probability nature of the integrity errors, it is not economically viable to save all data transfer monitoring statistics as the majority of them are normally successful. In the follow-up study, we will adopt a probabilistic approach that takes the statistic distribution of file transfer corruption and network component failures.      

As there are a large number of different models and associated parameters to be tuned, in this paper, we will not try to exhaust all the models and extensive parameter tuning. Rather, we choose the following three types of supervised learning models that have proved suitable for multi-class classification in the literature. We use the popular Scikit-learn library to implement these methods and used the default parameters in training these models~\cite{Scikit:web}. 

{\bf Decision Tree.}  Decision tree is a natural choice to multi-class classification as the multiple leaves represent the labels. It supports a $predict\_proba$ method that gives the class membership probability estimates. One of its main advantages is the fast prediction time after the tree is trained. In this study, we tried several ensemble methods based on randomized decision trees or random forests. By fitting over multiple randomized decision trees built from randomized samples, the random forest model achieves higher accuracy and controls overfitting. 

{\bf Support Vector Machine (SVM).} When using SVM for multi-class classification, the ``one-against-one" approach is adopted. As such, the training may take a long time to converge when the data set is big or the feature set is big. We experimented with both linear SVC in which the multiclass support is handled according to a one-vs-the-rest scheme and the general SVC with the default RBF kernel in which the multiclass support is handled according to a one-vs-one scheme.  SVC models in general cost much longer time in training and demonstrate subpar accuracy performance in dealing with mixed numerical and categorical features.  We use a \emph{CalibratedClassifierCV} implementation in {\it scikit-learn} library to generate the probability distribution.

{\bf Bayesian Networks (BN).} Since our ultimate goal is to infer the cause of the failures, BN is a model that is worth investigating. Among a number of models with different kernels, we chose the Multinomial Naive Bayes method, which is suitable for multi-class classification with discrete features. Again its $predict\_proba$ method gives the class membership probability estimates.

\subsection{Aggregated flows and inference accuracy}
One key observation is that an erratic link may cause integrity errors on all data transfer paths traversing it. While the training data is collected in the form of individually labeled flows, the inference can be done in the unit of all flows that are corrupted at a time since we only consider the single failure scenario. The models are trained with the labeled flow data.  For inference, we consider two different methods: {\it Flow} that just uses individual flow as the input and {\it Aggregated Flow} for which we generated a new data set that all corrupted file transfer flows at a time are aggregated as one input data sample. In the former case, the accuracy is computed on a per flow base. In the latter case, if all the flows in a data set are labeled by $L$ labels, they will be aggregated into $L$ samples to be tested against the trained model. The total number of correct label inference divided by $L$ is defined as the accuracy.

\subsection{Top-$k$ classification accuracy} 
Since we assume training data from data transfer flows only between the end hosts, it doesn't satisfy the necessary condition presented in~\cite{netbouncer:nsdi18}. The conventional classification on a single label inference from the training models performs relatively poorly in terms of accuracy and F-score. In practice, it would be very useful if the model can produce a small set of highly likely causes for the operators to zoom in. Most of the ML models, when used to infer a test sample, actually generate the probability distribution over all the classes. Therefore we can use a Top-$k$ Accuracy metric in evaluation, for which a prediction is defined as correct as long as the set of $k$ labels of highest probability in the classification results contains the correct label of the sample. Both decisions tree and BN models natively support the classification probability output. 

\subsection{Features}
As explained on Equation~\ref{eq:prob}, for a data sample, the feature set could consist of both path features and file transfer features. In our model, we only consider the features that are possible for the application to collect at the end hosts.
So only the end host information is included for the path features because we assume the other network elements on the file transfer paths are unknown. 
The file transfer features include both numerical characteristics like file size and transfer throughput , and categorical features like correctness of integrity checksum and presence of retransmission. 

The impacts of the file transfer features are two-folded. On one hand, the bigger file size may incur a higher probability of file corruption and lower throughput may imply more TCP retransmission caused by corrupted packets, which may help with the RCA performance. On the other hand, different machine learning models perform differently when dealing with a mixture of numerical and categorical features. In reality, there are always engineering and policy limits on obtaining certain 
features for application users in a network. Therefore it is important to study the model performance when only a subset of features are available. Therefore we study two scenarios of different feature sets: {\it No File Features} when the numerical file size and transfer throughput information is not available and {\it All Features} when it is available.

\subsection{Error Asymmetry and Data Imbalance} 
\label{sub:ml:imbalance}
A leading factor that affects the learning and prediction performance of classification models, esp. the multi-class classification models, is the problem of imbalanced data set. When data samples from certain classes (called {\it majority classes}) outnumber those from other classes, the trained models will be highly skewed toward the major classes, which will significantly lower the prediction accuracy. By the nature of integrity error simulation, the file transfer failures caused by faulty network interfaces are more frequent than those caused by the faulty end hosts. And between the two interfaces on a link, the one on the receiving side of a file transfer over TCP has a much higher chance to corrupt the file than the one on the transmitting side. Therefore the raw data we collected is oversampled on a subset of the network interface classes and significantly undersampled on the end host classes. We compared a number of data balancing techniques to augment the training data. The results show the random oversampling method can improve the model accuracy significantly.