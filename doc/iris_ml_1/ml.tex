Due to the scalability challenge, some recent work in large scale data center networks (possibly thousands of nodes) adopted stochastic learning approaches in localizing tomography based performance downgrades or probabilistic grey failure issues. Basically, they attempted to learn an estimate regression model between the output variables in the end-to-end performance measurements and the root causes as the input variables in either performance degradation at the end hosts~\cite{NetPoirot:Sigcomm2016} or gray failure probabilities inside the network nodes or links~\cite{netbouncer:nsdi18,Link-JIoT-2019}. Both inputs and outputs are explicitly defined as continuous variables. Their technical challenges are the regression model optimization algorithms (e.g., regularization and gradient methods) and statistical significance test~\cite{DeepView:NSDI18}. 

Our targeted networks are of multi-domain nature where the RCA granularity can be limited to individual domains instead of individual routers inside a domain, which are totally unknown to the outside. The network can be represented as a simple graph $G(V,E)$ where $V$ is the set of nodes that includes $H$ end hosts and $R$ routers. $E$ is the set of links. We say a file transfer succeeds when it incurs no integrity errors. 

In general, the problem at hand can be concisely represented by the following formula.
\begin{flalign}\label{eq:prob}
\begin{aligned}
&Probability(File\ i\ succeeds) =\\
&f(F_i, \prod_{j \in P_i}Probability(component\ j\ is\ normal) )
\end{aligned}
\end{flalign}

A path $P_i$ that a file transfer (flow) $i$ traverses consists of origin node $H_i^o$, a set of links where each $e_i^j\in E$ has two interfaces $e_i^t$ and $e_i^r$ on the route, and destination node $H_i^d$. As our main concern is if a file is corrupted rather than packet losses, the file characteristics, $F_i$, eg, the file size, transfer time, etc. may play an important role. We emphasize again that the components ($j$) of $P\ i$ that file transfer $i$ traverses is unknown except for its two end hosts $H_i^o$ and $H_i^d$. And we need large amount of data transfer flows to generate sufficient training data since our targeted grey failure, the integrity error, has very low probability (often in the order of $10^{-3}$).

We model our problem as a multi-class classification problem where the labels are defined as all the nodes and links that may incur integrity errors and the features are flow level characteristics that include source, destination, size, transfer time, throughput, whether a flow is corrupted, missed, or retried, etc. In general, the training process takes as input two arrays: an array X of size $[n_{samples}, n_{features}]$ holding the training samples, and an array y of class labels of size $[n_{samples}]$. The total number of labels is the number of the link interfaces and the end hosts in the topology, $L=2*|E|+|H|$.

As there is a large number of different models and associated parameters to be tuned, in this paper, we will not try to exhaust all the models and extensive parameter tuning. Rather, we choose the following three supervised learning models that have proved suitable for multi-class classification in the literature. We use the popular Scikit-learn library to implement these methods and used the default parameters in training these models. 

{\bf Decision Tree.}  Decision tree is a natural choice to multi-class classification as the multiple leaves represent the labels. Its $predict\_proba$ method gives the class membership probability estimates. One of its main advantages is the fast prediction time after the tree is trained. In this study, we tried several ensemble methods based on randomized decision trees or random forests. By fitting over multiple randomized decision trees built from randomized samples, the random forest model achieves higher accuracy and controls overfitting. 

{\bf Support Vector Machine (SVM).} When using SVM for multi-class classification, the ``one-against-one" approach is adapted. As such, the training may take long time to converge when the data set is big or the feature set is big. However, it doesn't have the native method to calculate probability distribution over all the classes. The $decision\_function$ method of SVC (Support Vector Classification) produces per-class confidence scores for each sample. We use the \emph{CalibratedClassifierCV} class of {\it scikit-learn} library in the evaluation. We experimented with both linear SVC and the general SVC with the default RBF kernel.

{\bf Bayesian Networks (BN).} Since our ultimate goal is to infer the cause of the failures, BN is a model that is worth investigating. Specifically, we use the Multinomial Naive Bayes method, which is suitable for the multi-class classification. Again its $predict\_proba$ method gives the class membership probability estimates.

\subsection{Data sample definition and training accuracy}
One key observation is that an erratic link may cause integrity errors on all paths traversing it. While the training can be done with the set of individual flows, inference is better to be done in the unit of all aggregated flows that are affected by a particular cause, i.e., a particular label. As a result, the training accuracy should be computed with the same unit of aggregated flows. As there are $L$ labels, all the labeled data (one per flow) will be aggregated into $M$ instances to be tested against the trained model. The resulting total number of correct label matches divided by $L$ is defined as the accuracy.

\subsection{Top-$k$ classification accuracy} 
Since we assume training data from data transfer flows only between the end hosts, it doesn't satisfy the necessary condition presented in~\cite{netbouncer:nsdi18}. The conventional classification on a single label resulted trained models perform relatively poorly in terms of accuracy and F-score. In practice it would be very useful if the model can produce a small set of highly likely causes for the operators to zoom in. Therefore we used a Top-$k$ Accuracy metric in evaluation, for which a prediction is defined as correct as long as the set of $k$ labels of highest probability in the classification results contains the correct label of the sample.

\subsection{Error Asymmetry and Data Imbalance} 
\label{sub:ml:imbalance}
A leading factor that affects the learning and prediction performance of classification models, esp. the multi-class classification models, is the problem of imbalance data set. When data samples from certain classes (called {\it majority classes}) outnumber those from other classes, the trained models will be highly skewed toward the major classes, which will significantly lower the prediction accuracy. By the nature of integrity error simulation, the file transfer failures caused by faulty network interfaces are more frequent than those caused by the faulty end hosts. And between the two interfaces on a link, the one on the receiving side of a file transfer over TCP has a much higher chance to corrupt the file than the one on the transmitting side. Therefore the raw data we collected is oversampled on a subset of the network interface classes and significantly undersampled on the end host classes. We thereafter applied a number of data balancing techniques before model training. The results show the random oversmapling scheme improved the model accuracy significantly.