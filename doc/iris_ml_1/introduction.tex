Root cause analysis (RCA) is a critical function in operating and managing complex networked systems, be it physical, software, or hybrid~\cite{RCA-Review-2017}.
It aims to identify the component(s) and process(es) responsible for the fault manifested by the wrong results or system failures in a timely fashion.
Traditional RCA relies on system domain models that can be used to deduce the potential component failures from the system symptoms and behaviors.
However, with the exponential increase of system scales, the complex of component interdependencies, and the lack of visibility in multi-domain, large-scale networks have 
significantly made it harder to build such models for efficient fault identification and localization for modern distributed systems. 
As a result, RCA in such a distributed and opaque system setting has drawn extensive research attention in recent years, which have found prominent uses
 in data center networks and Internet applications. Not surprisingly, these studies have adopted the machine learning (ML) or data analytic approaches
due to the relaxed requirements on accurate domain models~\cite{netbouncer:nsdi18,Link-JIoT-2019,microrca:noms2020}.

In this paper, we take a new application domain, the scientific workflow management systems (WMS), as our primary motivating use case to tackle the complex
 system RCA problem using a machine learning approach. WMS facilitates in-order execution of jobs in workflows and includes large amounts of interdependent
  data transfers, storage functions, and computation tasks. These tasks are often distributed over distributed hardware, software, and data resources
   located in different facilities nationwide or globally. Inevitably, frequent system failures and reliability issues 
caused by errors and faults from underlying subsystems have been serious concerns for the WMS community. 
Therefore most WMS have built-in failure handling mechanisms like redundancy and automatic retries. 
They also try to provide as much log information as possible to help with failure diagnosis, which normally assumes a long manual process.

Since a typical workflow system runs as a middleware sitting several layers above the infrastructure resources that are managed 
and operated by different service providers (domains), it has a limited view of the health of the infrastructure components. 
Most critically, it has no direct knowledge of the exact topology of the network and the routing path of the data movement,
sometimes even the sources and sinks are normally abstracted in virtual namespaces.  
Consequently, fault diagnosis and root cause analysis of the failures become extremely difficult and normally cost coordinated efforts and long hours from many operators of different sub-systems. Those failures often include many unsuccessful (and wasted) retries from the users. 
Hence, analyzing the root cause of failures for data integrity errors in distributed workflow executions is a representative yet very difficult problem.

In a nutshell, ML-based RCA can be formulated as a multi-class classification problem, where the potential root causes are the labels and various 
measurement and observations of the tasks and data flow level observations are used as the features.
RCA for large-scale networked systems poses other unique challenges in adopting ML approaches.
 
The first challenge comes from the prevalence of so-called {\it gray failures} in the networked system, in addition to the normal {\it stop failures}~\cite{GrayFailure:2017,DeepView:NSDI18}. A {\it stop failure} is a kind of hard failure, which refers to the complete breakdown of a component that disrupts all traffic flows or paths over this component deterministically~\cite{Link-JIoT-2019}. {\it Gray failures} are those probabilistic failures in a component that would act normally most of the time and that could not be caught by the traditional deterministic system monitoring and diagnostic tools. We categorize two types of  {\it gray failures}: performance degradation and data integrity errors. The former is normally caused by system overload that will lead to slow response, timeout, and the frequent reboot of the servers or software. The latter may randomly corrupt bits in a block of data or packets over network transfer. Since existing checksum mechanisms implemented in TCP and the storage services are not sufficient to guarantee end-to-end data integrity, they often get unnoticed for a long time until severe consequences to applications occurred. Therefore modern WMS have started to add end-to-end integrity check mechanisms, including in Pegasus~\cite{swip:pearc:2019} and Globus~\cite{IntegrityVerification:DataTransfer}.

The second challenge lies in the difficulties in acquiring sufficient training data feed to an ML-based RCA system. For a system over the Internet, the global routing information is not completely available for the RCA system as they are normally proprietary to different service providers (domains). The possible monitoring data sources or active probe sites in a network are always limited. As a result, in addition to passive monitoring data, active probes or event fault injections are often used to generate more diagnosis data~\cite{active:iot:2019, NetPoirot:Sigcomm2016}. Then, due to the desire of conducting RCA in real-time, how to minimize the overhead and latency of data collection in a production setting becomes a significant design issue.

The last but not the least challenge is to design and train the right ML models to achieve high diagnosis accuracy, out of a large pool of candidates~\cite{Boutaba:2018aa}. We will show that even the most basic questions of defining a data point, feature selection, and performance evaluation need special consideration.

In~\cite{Link-JIoT-2019}, the authors attempted to identify the stop failure of network links via the popular multi-class ML models using end-to-end passive traffic engineering measurements (throughput, latency, and packet loss). The authors in \cite{DeepView:NSDI18} took an active probe approach to localize the fault in a virtual disk system to the finest granularity up to the network switches. In~\cite{netbouncer:nsdi18}, a necessary condition was derived on the minimal set of paths that active probes need to be sent over the targeted network. Another line of work including~\cite{NetPoirot:Sigcomm2016,KDD14} adapted a statistical learning approach to infer the probabilistic relationship between the path failure and the link faults. All these research works made a strong assumption that one can instrument probes or observations from any node to any other node in the network since their target systems are data center networks that they own. In an earlier study, the decision tree model was used to predict if a request will fail or succeed over a flawed network system~\cite{DT:2004}. Bayesian inference was demonstrated to be efficient for fast diagnosis when the causal relationship model is established in a large Internet system~\cite{BN-Internet:2007}.

In this paper, we first cast the WMS integrity error diagnosis as a networked system Root Cause Analysis (RCA) problem, where data transfers only occur between end hosts and no flow routing and network monitoring information is available (Section~\ref{sec:integrity}). Our only assumptions on the network underlay are that the elements in the network are known and the file integrity at the end hosts of data transfers can be checked. In order to obtain sufficient training data and make experiments efficient and repeatable, we created an experimental system in a cloud testbed that can automatically create a large-scale OSPF-enabled virtual network system, initiate data transfers between end hosts, and inject arbitrary integrity errors into the virtual router interfaces and end hosts (Section~\ref{sec:emulation}). We then studied several variants from three different families of multi-class classification models, Bayesian inference, SVM, and Decision Tree. We identified that using the network-wide aggregated data flow as the input and a Top-$k$ accuracy metric can significantly improve the inference performance (Section~\ref{sec:ml}). With the data collected from the emulation, we evaluated the model performance in Section~\ref{sec:evaluation}. We specifically quantified the impacts of training data coverage in terms of total flows between all or part of the end hosts, mixed numerical and categorical features, and the inherited data imbalance in the training data set. For the emulated network system, the results show that a random forest model with the right set of training data and inference method can precisely localize the root cause of integrity errors to the single network interface or end host. The analysis validated our main hypothesis that the mapping between the application level flow characteristics and the network component failures for RCA can be learned and inferred with high accuracy from a sufficiently large amount of labeled training data. We conclude the paper in Section~\ref{sec:future}.
