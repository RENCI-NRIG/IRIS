Root cause analysis (RCA) is a critical function in operating and managing complex networked systems, be it physical, software, or hybrid~\cite{RCA-Review-2017}.
It aims to identify the component(s) and process(es) responsible for the fault manifested by the wrong results or system failures in a timely fashion.
Traditional RCA relies on accurate system models to deduce the potential component failures from the system symptoms and behaviors.
However,  it proves impossible to build such models for efficient fault identification and localization in complex networked systems. 
As a result, RCA for those systems largely remains a guessing art that requires intensive manual debugging using traditional tools 
like {\it ping} and {\it traceroute} and daunting amount of communication between operators from different organizations 
that often takes days or even weeks.

In this paper, we focus on the root cause analysis of one important failure mode, data integrity errors, in large-scale networked systems. These systems typically run 
over the Internet or dedicated wide area networks that are managed by multiple middleware and infrastructure service providers in a distributed fashion. 
Due to the insufficient capability of existing reliable protocols (TCP, , encrypted transfer and RAID, \et),  incomplete end-to-end data integrity coverage, and 
more possible component bugs, data integrity errors can stay stealth or be very difficult to be localized for long time.  
Driven by the exponentially growing data volumes and the sheer scale of the network, detecting and localizing the data integrity errors have garnered 
lots of research and development interests in recent years~\cite{tcp:ccr2000,swip:pearc:2019}. 

Data integrity errors carry the attributes of the so-called {\it gray failures} and {\it silent failure} in the networked system. This is in contrast to the so-called {\it hard failures} 
that cause data loss or corruption permanently. Recent studies showed that {\it gray failures}, of the probabilistic nature  causing performance degradation in terms 
of packet losses and latency could be efficiently localized using the machine learning (ML) approach~\cite{GrayFailure:2017,DeepView:NSDI18}. The data integrity errors, 
on the other hand, may randomly corrupt bits in a block of data or packets over network transfer, that can evade the existing checksum mechanisms implemented in TCP 
and the storage system. Due to its low error probability and silent nature in which it may result in both performance degradation and just wrong results, detection and localization 
of data integrity error become very challenging. Modern middleware systems have just started to add end-to-end integrity check mechanisms, 
for example, Pegasus~\cite{deelman-fgcs-2015} and Globus~\cite{IntegrityVerification:DataTransfer}, which efficiently addressed the detection problem. 

As it is not possible to build a complete system model, machine learning has become the leading candidate to develop RCA system for complex networks~\cite{Boutaba:2018aa}.  
In~\cite{Link-JIoT-2019}, the authors attempted to identify the hard failures of network links via the popular multi-class ML models using end-to-end 
passive traffic engineering measurements (throughput, latency, and packet loss). The authors in \cite{DeepView:NSDI18} took an active probe approach to localize the fault in a 
virtual disk system to the finest granularity up to the network switches. In~\cite{netbouncer:nsdi18}, a necessary condition was derived on the minimal set of paths 
that active probes need to be sent over the targeted network. Another line of work including~\cite{kdd14,detector:atc17,arzani2018democratically} adopted statistical 
learning approach to infer the probabilistic relationship between the path failure and the link faults. All these research works made a strong assumption that the RCA system 
can instrument probes or obtain measurements from any pairs of nodes in the network to obtain both packet-level routing path and measurement information. In an earlier study, 
the decision tree model was used to predict if a request will succeed over a flawed network system~\cite{DT:2004}. Bayesian inference was demonstrated to be efficient for fast 
diagnosis when the causal relationship model is established in a large Internet system~\cite{BN-Internet:2007}. A recent study focused on a source based measurement framework 
to diagnose the issues in the remote application services and therefore does not directly address the network components~\cite{microrca:noms2020}.

When developing an efficient RCA system, there are multiple dimensions in the design space. Different from the recent work in 
gray or hard failure RCA  in a production data center or single domain network, we target the large scale networked systems 
where there are more limitations in obtaining complete wide-area network information. The accurate network topology and 
routing information is normally unavailable as the {\it ping} and {\it traceroute} are always turned off in many domains due to security concerns 
from the service providers~\cite{topology_obf_20}. It is also not realistic to deploy and operate a comprehensive probe instrumentation 
and measurement system in this typical multi-domain environment with continuous network coverage like in the data center networks~\cite{guo2015pingmesh}. 

Therefore we take an application-centric view, rather than the infrastructure-centric view, to develop 
a machine learning RCA system to localize the data integrity errors. Our basic idea is to formulate the targeted RCA problem as a multi-class classification problem, 
where the potential root causes can be classified using flow level measurements provided by the application layer, augmented by limited network information.
And we assume only passive measurement on the flows between the end hosts is possible from the application layer. 

We specifically use a workflow management system (WMS)~\cite{deelman-fgcs-2015}, a popular Internet-scale distributed application, as the targeted system. 
WMS facilitates in-order execution of jobs in workflows and includes large amounts of interdependent
  data transfers, storage functions, and computation tasks. These tasks are often distributed over distributed hardware, software, and data resources 
  located in different facilities nationwide or globally. Inevitably, frequent system failures and reliability issues 
caused by errors and faults from underlying subsystems have been serious concerns for the WMS community. 

The rest of this paper is organized as follows.We first define the network system integrity error RCA problem with incomplete system and measurement information in Section~\ref{sec:integrity}. 
We present our machine learning approach and model selection in Section~\ref{sec:ml}. We identified that using the network-wide aggregated data flow as the input, training data balancing, 
and a Top-$k$ accuracy metric can significantly improve the classification accuracy.  Then a high-fidelity system emulation environment in a cloud testbed is introduced in Section~\ref{sec:emulation}. 
We evaluated the system performance from the extensive emulation of a representative network in Section~\ref{sec:evaluation}. The paper is concluded in Section~\ref{sec:future}.
