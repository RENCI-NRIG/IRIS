Root cause analysis (RCA) is a critical function in operating and managing complex networked systems, be it physical, software, or hybrid~\cite{RCA-Review-2017}.
It aims to identify the component(s) and process(es) responsible for the fault manifested by the wrong results or system failures in a timely fashion.
Traditional RCA relies on system domain models that can be used to deduce the potential component failures from the system symptoms and behaviors.
However, exponentially increasing system scale, complex component interdependencies, and lack of visibility in multi-domain, large-scale networks have 
significantly made it harder to build such models for efficient fault identification and localization for modern distributed systems. 
As a result, RCA in such a distributed and opaque system setting has drawn extensive research attention in recent years, which have found prominent uses
 in data center networks and Internet applications. Not surprisingly, these studies have adopted the machine learning (ML) or data analytic approaches
due to the relaxed requirements on accurate domain models~\cite{netbouncer:nsdi18,Link-JIoT-2019}.

In this paper, we take a new application domain, the scientific workflow management systems (WMS), as our primary motivating use case to tackle the complex
 system RCA problem using a machine learning approach. WMS facilitates in-order execution of jobs in workflows and includes large amounts of interdependent
  data transfers, storage functions, and computation tasks. These tasks are often distributed over distributed hardware, software, and data resources
   located in different facilities nationwide or globally. Inevitably, frequent system failures and reliability issues 
caused by errors and faults from underlying subsystems have been serious concerns for the WMS community. 
Therefore most WMS have built-in failure handling mechanisms like redundancy and automatic retries. 
They also try to provide as much log information as possible to help with failure diagnosis, which normally assumes a long manual process.

Since a typical workflow system runs as a middleware sitting several layers above the infrastructure resources that are managed 
and operated by different service providers (domains), it has a limited view of the health of the infrastructure components. 
Most critically, it has no direct knowledge of the exact path of the data movement,
sometimes even the sources and sinks are normally abstracted in virtual namespaces.  
Consequently, fault diagnosis, especially identifying the root cause of the failures becomes extremely difficult, and normally 
costs coordinated efforts and long hours from many operators of different sub-systems, often after many unsuccessful (and wasted) retries from the users. 
Hence, analyzing the root cause of failures for data integrity errors in distributed workflow executions is a representative yet very difficult problem.

In a nutshell, ML-based RCA can be formulated as a multi-class classification problem, where the potential root causes are the labels and various 
measurement and observations of the tasks and data flow level observations are used as the features.
RCA for large-scale networked systems poses other unique challenges in adopting ML approaches.
 
The first challenge comes from the prevalence of so-called {\it gray failures} in the networked system, in addition to the normal {\it stop failures}~\cite{GrayFailure:2017,DeepView:NSDI18}. A {\it stop failure} is a kind of hard failure, which refers to the complete breakdown of a component that disrupts all traffic flows or paths over this component deterministically~\cite{Link-JIoT-2019}. {\it Gray failures} are those probabilistic failures in a component that would act normally most of the time and that could not be caught by the traditional deterministic system monitoring and diagnostic tools. We categorize two types of  {\it gray failures}: performance degradation and data integrity errors. The former is normally caused by system overload that will lead to slow response, timeout, and the frequent reboot of the servers or software. The latter may randomly corrupt bits in a block of data or packets over network transfer. Since existing checksum mechanisms implemented in TCP and the storage services are not sufficient to guarantee end-to-end data integrity, they often get unnoticed for a long time until severe consequences to applications occurred. Therefore modern WMS have started to add end-to-end integrity check mechanisms, including in Pegasus~\cite{swip:pearc:2019} and Globus~\cite{IntegrityVerification:DataTransfer}.

The second challenge lies in the difficulties in acquiring sufficient training data feeds to a ML-based RCA system. For a system over the Internet, the global routing information is not completely available for the RCA system as they are normally proprietary to the different service providers (domains). The possible monitoring data sources or active probe sites in a network are always limited. As a result, in addition to passive monitoring data, active probes or event fault injections are often used to generate more diagnosis data~\cite{active:iot:2019, NetPoirot:Sigcomm2016}. Then, due to the desire of conducting RCA in real-time, how to minimize the overhead and latency of data collection in a production setting becomes a significant design issue.

The last but not the least challenge is to design and train the right ML models to achieve high diagnosis accuracy, out of a large pool of candidates~\cite{Boutaba:2018aa}. We will show that even the most basic questions of defining a data point, feature selection, and performance evaluation need special consideration.

In~\cite{Link-JIoT-2019}, the authors attempted to identify the stop failure of network links via the popular multi-class ML models using end-to-end passive traffic engineering measurements (throughput, latency, and packet loss). The authors in \cite{DeepView:NSDI18} took an active probe approach to localize the fault in a virtual disk system to the finest granularity up to the network switches. In~\cite{netbouncer:nsdi18}, a necessary condition was derived on the minimal set of paths that active probes need to be sent over the targeted network. The line of work including~\cite{NetPoirot:Sigcomm2016,KDD14} adapted a statistical learning approach to infer the probabilistic relationship between the path failure and the link faults. All these research works made a strong assumption that one can instrument probes or observations from any node to any other node in the network since their target systems are data center networks that they own. In an earlier study, the decision tree model was used to predict if a request will fail or succeed over a flawed network system~\cite{DT:2004}. Bayesian inference was demonstrated to be efficient for fast diagnosis when the causal relationship model is established in a large Internet system~\cite{BN-Internet:2007}.

In this paper, we first cast the WMS integrity error diagnosis as a networked system Root Cause Analysis (RCA) problem, where limited network system information is available for end-to-end data transfers (Section~\ref{sec:integrity}). Our main hypothesis is that the mapping between the endhost level flow statistics and the possible network component failures can be learned and inferred from a sufficiently large amount of labeled training data. In order to obtain sufficient training data and make repeatable experiments more efficient, we created an experimental system in a Cloud testbed that can automatically create a large-scale OSPF-enabled virtual network system, initiate data transfers between end hosts, and inject arbitrary integrity errors into the virtual router interfaces and end hosts (Section~\ref{sec:emulation}). We then studied several variants from three different families of multi-class classification models, Bayesian inference, SVM, and Decision Tree, and validated their accuracy performance using a Top-$k$ accuracy metric (Section~\ref{sec:ml}). With the data collected from the emulation, we evaluate their accuracy in Section~\ref{sec:evaluation}. We specifically studied the impact of available data transfer data in terms of end host pair coverage. We conclude the paper with our future research plan in Section~\ref{sec:future}.
