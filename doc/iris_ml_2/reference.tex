
"prior distribution -> posterior distribution" : "1.  reduce overfitting from small data set by bringing in prior distribution and knowledge; 2. scalability of ML; 3. the ability to express the uncertainty of predictions is one of the most important capabilities of Bayesian learning".

"Deep exponential families (e.g., deep latent Gaussian models)", "Gaussian process models (e.g., regression / classification)", "reinforcement learning (Bayesian Reinforcement Learning) "

"For classification problems we wish to construct models
that are able to accurately to predict categorical output. For a machine learning classification algorithm that produces probabilistic output,
one approach is to model the probability mass function of the output given the input"~\cite{RW06:GP}.

"Consider a case where we are given inputs x1, . . . , xn ? $X_i$ with corresponding outputs y1, . . . , yn ? {?1, 1}. In this work we will assume that X ? Rd, although in reality the inputs could have categorical components. Here yi is referred to as the class or output corresponding to the input xi, and D = {(x1, y1), . . . , (xn, yn)} is called the training dataset since it is the data that will be used to train a classifier."

Two common choices of prior distribution: the cumulative distribution functions of the standard normal distribution and the standard logistic distribution. In the context of Gaussian process classification they are known as the cumulative Gaussian likelihood function and the logistic likelihood function.

~\cite{gp:on:2018}, As was pointed out, the examined fault localization scheme consists of two phases. The first phase is activated upon the detection of a failure and consists of correlating the affected and unaffected lightpaths. For the path correlation phase, the GBC heuristic [13] is used. The second phase is activated only if the GBC heuristic reports that multiple links are suspected of causing the failure. This phase consists of computing a failure probability for each suspect link. The failure probability is computed by a GP classifier trained on a set that describes past failure incidents. 

the MTBF of each link in the network follows the Weibull distribution. we count (1) the total number of failures in the network $C(n)$ and (2) the number of failures $c_{j}$ associated with each link $e_j$ in the network, up to the last known failure incident $i$. Then, by dividing $C_j(n)$ by $C(n)$ for each incident $i$, we can compute the failure rate of each link $j$ at each time point.

Such historical information can then be utilized by a probabilistic model to find the likelihood that a link $e_j$ is the failed link upon incident i. The class of GPs is one of the most widely used families of stochastic processes for modeling dependent data observed over time. Thus, GPs are useful for sequential data, such as time-series and tracking applications, and can be used in particular for active data selection in such systems [16]. GPs constitute one of the most important Bayesian ML approaches and are based on a particularly effective method for placing a prior distribution over the space of regression functions. They have a small number of tunable parameters, can be trained on relatively small training sets, and allow us to capture nonlinear and skewed artifacts, thus exhibiting significant robustness to outliers and the ability to handle sparse data without becoming prone to overtraining. Compared to another popular form of discriminative kernel machines, i.e., the support vector machine (SVM) [17], GPs possess several advantages, with the most significant being that the GP model produces an output with a clear probabilistic interpretation, providing a measure of uncertainty for the obtained predictions, unlike SVMs, which merely provide point predictions [18].
------------------------

~\cite{Boulle:2019aa}
Boullé, M., Charnay, C.  Lachiche, N. A scalable robust and automatic propositionalization approach for Bayesian classification of large mixed numerical and categorical data. Mach Learn 108, 229,266 (2019).

We introduce a propositionalization approach dedicated to a robust Bayesian classifier. It efficiently samples a given number of features in the language bias, following a distribution over the complex aggregates.  
using aggregation functions such as minimum, maximum or average in order to deal with both the numerical attributes and the one-to-many relationship, whether existing relational data mining systems can be used on large mixed, numerical and categorical, data, and in particular whether increasing the expressivity by using complex features increases overfitting too and whether their runtimes and memory consumption are acceptable.
Propositionalization transforms the relational data into an attribute-value dataset in order to use attribute-value learners the data scientists may be familiar with. 
? Count(Table)?Num count of records in a table,
? Mode(Table, CatFeat)?Cat most frequent value of a categorical feature in a table,
? CountDistinct(Table, CatFeat)?Num number of distinct values,
? Mean(Table, NumFeat)?Num mean value of a numerical feature in a table, ? Median(Table, NumFeat)?Num median value,
? Min(Table, NumFeat)?Num min value,
? Max(Table, NumFeat)?Num max value,
? StdDev(Table, NumFeat)?Num standard deviation, ? Sum(Table, NumFeat)?Num sum of values.

Our proposition for solving these problems is the introduction of an evaluation criterion of the constructed features according to a Bayesian approach in order to penalize complex features. This is called regularization. In order to implement this evaluation criterion, we propose a prior distribution on the space of all features and an efficient sampling algorithm of the space of features according to their prior distribution. Our approach builds upon the MODL supervised preprocessing methods (Boullé 2005, 2006). These methods consists in partitioning either a numerical features into intervals or a cate- gorical feature into groups of values, through a piecewise constant class conditional density estimation.

Boullé (2006) for supervised discretization and Boullé (2005) for supervised value grouping.
------------------


The root causes of the failure may come from the network devices or the storage or compute servers at the edge clusters. The network devices and cluster servers demonstrate very different failure characteristics and are managed by different system administrative groups. More specifically, their probabilistic distributions are in different magnitude ranges.     

In this paper, we designed a two-step machine learning classification approach that combine a decision tree model to identify a subset of possible failure root causes and a statistical learning model to 
compute the probabilistic likelihood of these causes. 



In this paper, we first cast the WMS integrity error diagnosis as a networked system Root Cause Analysis (RCA) problem, where limited network system information is available for end-to-end data transfers (Section~\ref{sec:integrity}). Our main hypothesis is that the mapping between the end host level flow statistics and the possible network component failures can be learned and inferred from sufficiently large amount of labeled training data. In order to obtain sufficient amount of training data and make repeatable experiments more efficient, we created an experimental system in a Cloud testbed that can automatically create a large-scale OSPF-enabled virtual network system, initiate data transfers between end hosts, and inject arbitrary integrity errors into the virtual router interfaces and end hosts (Section~\ref{sec:emulation}). We then studied several variants from three different families of multi-class classification models, Bayesian inference, SVM, and Decision Tree, and validated their accuracy performance using a Top-$k$ accuracy metric (Section~\ref{sec:ml}). With the data collected from the emulation, we evaluate their accuracy in Section~\ref{sec:evaluation}. We specifically studied the impact of available data transfer data in terms of end host pair coverage. We conclude the paper with our future research plan in Section~\ref{sec:future}.
