Assurance of data integrity has been one of the most fundamental aspects of networked system and Internet applications. 
Different mechanisms of error tolerance, detection, and mitigation have been widely implemented and deployed in different layers of 
the compute, storage, network, and applications systems. Unfortunately, these measures are not sufficient to cover the data corruptions 
in large-scale networks. For example, It is well known that the Ethernet CRC and TCP checksums are too small for 
modern data sizes~\cite{tcp:ccr2000}. Facebook recently reported a CPU bug that caused severe data corruptions in its hyper-scale data 
centers~\cite{facebook:cpu:2021}. 

%Due to its ``silent'' nature, data integrity errors can stay unnoticed for long time and therefore can lead to catastrophic results to the applications 
%with the corrupted data. While the probability of undetected integrity errors seems to be low, the proliferation of big data accompanied by 
%the ever-growing scales of Internet, Cloud, IoT (Internet of Things), and HPC (High-Performance Computer) in recent years has amplified 
%the challenges to maintain high data integrity for large-scale networked systems and distributed applications. 
%It was estimated that up to 1 in 10 billion TCP segments may encounter undetected TCP checksum error. A simple calculation tells that 10 billion maximum 
%Ethernet frames translates to only 3.4 hours of data transfer on a 10gbps (gigabit per second) link~\cite{tcp:ccr2000}. Therefore, efficient diagnosis of data corruption  
%errors becomes more essential than ever to ensure the application and data integrity.

Data integrity error is a representative ``gray'' network failure~\cite{GrayFailure:2017}, for which, 
network component faults are probabilistic and often evasive from the monitoring system. Gray failure diagnosis in large-scale networks 
is often a latent act after disastrous results from the applications and services. It remains a guessing art in most systems that requires intensive 
manual debugging and daunting amount of communication between operators 
from different organizations that often takes days or even weeks, primarily due to the infeasibility of accurate network models and 
incomplete coverage of system monitoring. 

In recent years, machine learning (ML) techniques have found phenomenal success in solving the failure diagnosis challenges.
The strength of ML models in learning the complex mapping between the failure root causes and the system level measurement 
observations effectively relieves the need on accurate domain models and full element level monitoring.  However, the majority of 
these systems targeted the data center networks, where detailed network topology and traffic routing information can be determined and 
costly active and passive measurement system can be deployed by the operators. Their main 
technical contributions have been developing scalable inference models and measurement system of complete coverage for 
fine classification or regression performance~\cite{netbouncer:nsdi18,DeepView:NSDI18,arzani2018democratically}.

Accurate and fast fault localization requires complete network coverage from the measurement system within a limited time window. 
From ML perspective, training and test data sets with complete features are required for the model and inference. 
This is why substantial efforts were made in the existing work to develop complex measurement system to ensure the network failure coverage. 

In this study, we target a completely different network environment, Internet scale network, where multiple networks of different 
administrative domains are used to support distributed applications. Different from the data center networks, while the scalability might be smaller 
depending on topological and diagnostic granularity, the challenges on the available system information and measurement data are 
severed by the multi-domain nature of the Internet and applications. In this typical ``opaque" network setting, deploying active probing 
to gather the network information and instrument the diagnosis in the production network is normally not feasible. It is also not realistic to deploy 
always-on passive monitoring system over the edges of the whole network.
 
On the other hand, modern Internet application software systems have built-in measurement and monitoring capabilities to ensure the application performance 
and mitigate the effects of failures in the network system layer~\cite{IntegrityVerification:DataTransfer,swip:pearc:2019,iris:ictc21}. 
From the gray failure diagnostic perspective, the fundamental ML based solution approach appears to be a very viable choice with its promise in 
learning a model to map the observations to the system internal behaviors. In our case, the design goal is application-centric: to infer the fault information at the 
component level from the application level measurement and monitoring information. 

A big challenge in end host based application-centric solution approach is the data completeness being discussed above. First of all, the end hosts deployed on the network 
by an application may not need all the network components in forwarding the traffic.  Even we limit our scope to exclude the part of network not being covered,  
in both training data set and testing data set, some features data may be missing in some data samples. In the extreme case, some features may be totally missed. 
This data missingness challenge is exacerbated to deal with the data integrity errors for which the failure rate is normally very low and the application traffic 
may be very imbalanced among the source and destination hosts at the edge of the network.
They may be caused by either incomplete coverage of the measurement, or the lost measurement records as the distributed measurement 
sub-system itself is not complete reliable in realtime, or there are just no measurements available for a part of application traffic during some time windows. 

Missing data has been a prominent research topic in statistics and is garnering more active research interests
 in ML applications in the areas of medical science~\cite{DONDERS20061087} and sensor applications~\cite{missingdata:sensor:20}.
 The simple imputation techniques, like using the basic statistics (zero, mean, min, max, etc) of the existing feature data does 
not apply to our network failure diagnosis model because of the strong dependency between the application flows (features) and the system components (targets).   
The more suitable choices are the multivariate imputation algorithms that use the whole feature space to estimate (impute) the missing data in particular features.

In this paper, we first present a new multi-output ML prediction model that directly maps the application level observations to localize the system component failures. 
This model not only captures the fact that one faulty component would cause failures in multiple application flows, but also naturally allow application of 
proven imputation methodologies to address the {\it missing data} challenge. Instead of pursuing more system information and complete measurement coverage, 
we focus on the multivariate imputation algorithms, parameter tuning, and quantifying their performance in improving the inference accuracy of failure localization.  
We also looked into the most recent algorithms based on the Generative Adversarial Nets (GAN) framework to generate the 
missing data~\cite{Yoon2018GAINMD,Awan2021ImputationOM}.

As far as we know, this is the first study on the missing data issue and applying imputation techniques in the area of network failure diagnosis. The evaluation results 
show satisfactory prediction accuracy. This model approach and missing data imputation results also present opportunities to the development and deployment of 
economical measurement capabilities in practical network settings.     









